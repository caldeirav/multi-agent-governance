{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Report Generation with AgentWorkflow\n",
    "\n",
    "In this notebook, we will explore how to use the `AgentWorkflow` class to create multi-agent systems. Specifically, we will create a system that can generate a report on a given topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this example, we demonstrate the use of local serving of a `qwen3-8b` Small Language Model (SML) as our LLM, served by LM Studio, and a state-of-the-art model `gemini-2.5-flash` hosted by Google Cloud. For all supported LLM inference providers and models, check out the [examples documentation](https://docs.llamaindex.ai/en/stable/examples/llm/openai/) or [LlamaHub](https://llamahub.ai/?tab=llms) for a list of all supported LLMs and how to install/use them.\n",
    "\n",
    "If we wanted, each agent could have a different LLM, but for this example, we will use the same LLM for all agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Environment variables for local LM studio inference\n",
    "model = \"qwen/qwen3-8b\"\n",
    "base_url = \"http://127.0.0.1:1234/v1\"\n",
    "api_key = \"\"\n",
    "\n",
    "# Environment variable for Google GenAI API inference\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
    "google_model = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for \"RuntimeError: This event loop is already running\"\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.llms.lmstudio import LMStudio\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "\n",
    "# Initialize the LMStudio client with the model and base URL\n",
    "#llm = LMStudio(\n",
    "#    model_name=model,\n",
    "#    base_url=base_url,\n",
    "#    temperature=0.7,\n",
    "#)\n",
    "\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "# Initialize the Google GenAI client with the API key\n",
    "llm = GoogleGenAI(\n",
    "    model=google_model,\n",
    "    api_key=google_api_key,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the LLM endpoint with a simple prompt\n",
    "response = llm.complete(\"What is 2+2?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Design\n",
    "\n",
    "Our system will have three agents:\n",
    "\n",
    "1. A `ResearchAgent` that will search the web for information on the given topic.\n",
    "2. A `WriteAgent` that will write the report using the information found by the `ResearchAgent`.\n",
    "3. A `ReviewAgent` that will review the report and provide feedback.\n",
    "\n",
    "We will use the `AgentWorkflow` class to create a multi-agent system that will execute these agents in order.\n",
    "\n",
    "While there are many ways to implement this system, in this case, we will use a few tools to help with the research and writing processes.\n",
    "\n",
    "1. A `web_search` tool to search the web for information on the given topic.\n",
    "2. A `query_engine` tool to query local documents via Query Engine (RAG)\n",
    "3. A `record_notes` tool to record notes on the given topic.\n",
    "4. A `write_report` tool to write the report using the information found by the `ResearchAgent`.\n",
    "5. A `review_report` tool to review the report and provide feedback.\n",
    "\n",
    "Utilizing the `Context` class, we can pass state between agents, and each agent will have access to the current state of the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index for the query engine tool from data sources in local ./data directory\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "try:\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=\"./storage/\"\n",
    "    )\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    index_loaded = True\n",
    "except:\n",
    "    index_loaded = False\n",
    "\n",
    "if not index_loaded:\n",
    "    # load data\n",
    "    docs = SimpleDirectoryReader(\n",
    "        input_dir=[\"./data/\"].\n",
    "        file_extractors=[\".txt\", \".pdf\"],\n",
    "    ).load_data()\n",
    "\n",
    "    # build index\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "\n",
    "    # persist index\n",
    "    index.storage_context.persist(persist_dir=\"./storage/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query engine tool\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "async def query_data(query: str) -> str:\n",
    "    \"\"\"Useful for querying local data on the history of the Internet\"\"\"\n",
    "    engine = index.as_query_engine(similarity_top_k=3)\n",
    "    return str(await engine.query(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import AsyncTavilyClient\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "\n",
    "async def search_web(query: str) -> str:\n",
    "    \"\"\"Useful for using the web to answer questions.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=os.environ.get(\"TAVILY_SEARCH_API_KEY\"))\n",
    "    return str(await client.search(query))\n",
    "\n",
    "\n",
    "async def record_notes(ctx: Context, notes: str, notes_title: str) -> str:\n",
    "    \"\"\"Useful for recording notes on a given topic. Your input should be notes with a title to save the notes under.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    if \"research_notes\" not in current_state:\n",
    "        current_state[\"research_notes\"] = {}\n",
    "    current_state[\"research_notes\"][notes_title] = notes\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Notes recorded.\"\n",
    "\n",
    "\n",
    "async def write_report(ctx: Context, report_content: str) -> str:\n",
    "    \"\"\"Useful for writing a report on a given topic. Your input should be a markdown formatted report.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    current_state[\"report_content\"] = report_content\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Report written.\"\n",
    "\n",
    "\n",
    "async def review_report(ctx: Context, review: str) -> str:\n",
    "    \"\"\"Useful for reviewing a report and providing feedback. Your input should be a review of the report.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    current_state[\"review\"] = review\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Report reviewed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our tools defined, we can now create our agents.\n",
    "\n",
    "If the LLM you are using supports tool calling, you can use the `FunctionAgent` class. Otherwise, you can use the `ReActAgent` class.\n",
    "\n",
    "Here, the name and description of each agent is used so that the system knows what each agent is responsible for and when to hand off control to the next agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent, ReActAgent\n",
    "\n",
    "test_agent_search = ReActAgent(\n",
    "    tools=[search_web],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can search the web for information.\",\n",
    ")\n",
    "\n",
    "test_agent_query = ReActAgent(\n",
    "    tools=[query_data],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can search the web for information.\",\n",
    ")\n",
    "\n",
    "research_agent = ReActAgent(\n",
    "    name=\"ResearchAgent\",\n",
    "    description=\"Useful for searching the web for information on a given topic and recording notes on the topic.\",\n",
    "    system_prompt=(\n",
    "        \"You are the ResearchAgent that can search the web for information on a given topic and record notes on the topic.\"\n",
    "        \"You should first search the web with the search_web tool for information on the topic and then record notes on the topic. \"\n",
    "        \"Once notes are recorded and once you are satisfied, you should always hand off control to the WriteAgent to write a report on the topic. \"\n",
    "        \"You should have at least some notes on a topic before handing off control to the WriteAgent.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[search_web, record_notes],\n",
    "    can_handoff_to=[\"WriteAgent\"],\n",
    ")\n",
    "\n",
    "write_agent = ReActAgent(\n",
    "    name=\"WriteAgent\",\n",
    "    description=\"Useful for writing a report on a given topic.\",\n",
    "    system_prompt=(\n",
    "        \"You are the WriteAgent that can write a report on a given topic. \"\n",
    "        \"Your report should be in a markdown format. The content should be grounded in the research notes. \"\n",
    "        \"Once the report is written, you should get feedback at least once from the ReviewAgent.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[write_report],\n",
    "    can_handoff_to=[\"ReviewAgent\", \"ResearchAgent\"],\n",
    ")\n",
    "\n",
    "review_agent = ReActAgent(\n",
    "    name=\"ReviewAgent\",\n",
    "    description=\"Useful for reviewing a report and providing feedback.\",\n",
    "    system_prompt=(\n",
    "        \"You are the ReviewAgent that can review the write report and provide feedback. \"\n",
    "        \"Your review should either approve the current report or request changes for the WriteAgent to implement. \"\n",
    "        \"If you have feedback that requires changes, you should hand off control to the WriteAgent to implement the changes after submitting the review.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[review_report],\n",
    "    can_handoff_to=[\"WriteAgent\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a single agent\n",
    "\n",
    "Use the test agent to ensure that agent and tools work with the chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await test_agent_search.run(user_msg=\"What is the weather in San Francisco?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await test_agent_query.run(user_msg=\"Who published the first paper on packet switching theory?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Workflow\n",
    "\n",
    "With our agents defined, we can create our `AgentWorkflow` and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "\n",
    "agent_workflow = AgentWorkflow(\n",
    "    agents=[research_agent, write_agent, review_agent],\n",
    "    root_agent=research_agent.name,\n",
    "    initial_state={\n",
    "        \"research_notes\": {},\n",
    "        \"report_content\": \"Not written yet.\",\n",
    "        \"review\": \"Review required.\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the workflow is running, we will stream the events to get an idea of what is happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentInput,\n",
    "    AgentOutput,\n",
    "    ToolCall,\n",
    "    ToolCallResult,\n",
    "    AgentStream,\n",
    ")\n",
    "\n",
    "handler = agent_workflow.run(\n",
    "    user_msg=(\n",
    "        \"Search for the history of internet and write me a report on it. \"\n",
    "        \"Briefly describe the history of the internet, including the development of the internet, the development of the web, \"\n",
    "        \"and the development of the internet in the 21st century.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "current_agent = None\n",
    "current_tool_calls = \"\"\n",
    "async for event in handler.stream_events():\n",
    "    if (\n",
    "        hasattr(event, \"current_agent_name\")\n",
    "        and event.current_agent_name != current_agent\n",
    "    ):\n",
    "        current_agent = event.current_agent_name\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ü§ñ Agent: {current_agent}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "\n",
    "    # if isinstance(event, AgentStream):\n",
    "    #     if event.delta:\n",
    "    #         print(event.delta, end=\"\", flush=True)\n",
    "    # elif isinstance(event, AgentInput):\n",
    "    #     print(\"üì• Input:\", event.input)\n",
    "    elif isinstance(event, AgentOutput):\n",
    "        if event.response.content:\n",
    "            print(\"üì§ Output:\", event.response.content)\n",
    "        if event.tool_calls:\n",
    "            print(\n",
    "                \"üõ†Ô∏è  Planning to use tools:\",\n",
    "                [call.tool_name for call in event.tool_calls],\n",
    "            )\n",
    "    elif isinstance(event, ToolCallResult):\n",
    "        print(f\"üîß Tool Result ({event.tool_name}):\")\n",
    "        print(f\"  Arguments: {event.tool_kwargs}\")\n",
    "        print(f\"  Output: {event.tool_output}\")\n",
    "    elif isinstance(event, ToolCall):\n",
    "        print(f\"üî® Calling Tool: {event.tool_name}\")\n",
    "        print(f\"  With arguments: {event.tool_kwargs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can retrieve the final report in the system for ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = await handler.ctx.store.get(\"state\")\n",
    "print(state[\"report_content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
